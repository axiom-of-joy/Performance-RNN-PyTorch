{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PerformanceRNN\n",
    "import torch\n",
    "from torch import nn\n",
    "import distiller\n",
    "from distiller.modules.gru import DistillerGRU as GRU\n",
    "from distiller.modules.gru import convert_model_to_distiller_gru\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and converting to our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = 'cuda:0'\n",
    "sess_path = \"save/ecomp_w500.sess\"\n",
    "state = torch.load(sess_path)\n",
    "rnn_model = PerformanceRNN(**state['model_config']).to(device)\n",
    "rnn_model.load_state_dict(state['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): Embedding(240, 240)\n",
       "  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): DistillerGRU(512, 512, num_layers=3, dropout=0.30, bidirectional=False)\n",
       "  (output_fc): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_model_to_distiller_gru(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that man_model is on GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(man_model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the conversion has succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to make sure that both the original and manual models can generate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = quantizer.model.to(device)\n",
    "model.eval()\n",
    "batch_size = 1\n",
    "init = torch.randn(batch_size, model.init_dim).to(device)\n",
    "max_len = 1000\n",
    "controls=None\n",
    "greedy_ratio = 0.7\n",
    "temperature = 1.0\n",
    "\n",
    "import pdb\n",
    "\n",
    "with torch.no_grad():\n",
    "    #pdb.set_trace()\n",
    "    outputs = model.generate(init, max_len,\n",
    "                             controls=controls,\n",
    "                             greedy=greedy_ratio,\n",
    "                             temperature=temperature,\n",
    "                             verbose=True)\n",
    "    \n",
    "\n",
    "outputs = outputs.cpu().numpy().T # [batch, steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert output to MIDI and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import os\n",
    "\n",
    "output_dir = \"quantized_output/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    name = f'output-{i:03d}.mid'\n",
    "    path = os.path.join(output_dir, name)\n",
    "    n_notes = utils.event_indeces_to_midi_file(output, path)\n",
    "    print(f'===> {path} ({n_notes} notes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect activation statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses activation statistics to determine how big the quantization range is. The bigger the range - the larger the round off error after quantization which leads to accuracy drop.  \n",
    "Our goal is to minimize the range s.t. it contains the absolute most of our data.  \n",
    "After that, we divide the range into chunks of equal size, according to the number of bits, and transform the data according to this scale factor.  \n",
    "Read more on scale factor calculation [in our docs](https://nervanasystems.github.io/distiller/algo_quantization.html).\n",
    "\n",
    "The class `QuantCalibrationStatsCollector` collects the statistics for defining the range $r = max - min$.  \n",
    "\n",
    "Each forward pass, the collector records the values of inputs and outputs, for each layer:\n",
    "- absolute over all batches min, max (stored in `min`, `max`)\n",
    "- average over batches, per batch min, max (stored in `avg_min`, `avg_max`)\n",
    "- mean\n",
    "- std\n",
    "- shape of output tensor  \n",
    "\n",
    "All these values can be used to define the range of quantization, e.g. we can use the absolute `min`, `max` to define the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `man_model` has the same weights as `rnn_model` (Warning: Running this will move the models to the CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.testing as nptest\n",
    "\n",
    "man_model_weights = man_model.cpu().output_fc.weight.detach().numpy()\n",
    "rnn_model_weights = rnn_model.cpu().output_fc.weight.detach().numpy()\n",
    "nptest.assert_array_almost_equal(man_model_weights, rnn_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `man_model` is on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(man_model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My version.\n",
    "import os\n",
    "from distiller.data_loggers import QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "# Commented line is probably not necessary.\n",
    "#man_model = torch.load('./manual.checkpoint.pth.tar')\n",
    "distiller.utils.assign_layer_fq_names(rnn_model)\n",
    "collector = QuantCalibrationStatsCollector(rnn_model)#,\n",
    "                                           #inplace_runtime_check=True,\n",
    "                                           #disable_inplace_attrs=True) # I added this last argument.\n",
    "\n",
    "# Random numbers.\n",
    "batch_size = 64\n",
    "max_len = 100\n",
    "\n",
    "if not os.path.isfile('performance_rnn_pretrained_stats.yaml'):\n",
    "    with collector_context(collector) as collector:\n",
    "        init = torch.randn(batch_size, rnn_model.init_dim).to(device)\n",
    "        output = rnn_model.generate(init, max_len)\n",
    "        collector.save('performance_rnn_pretrained_stats.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Model:\n",
    "  \n",
    "We quantize the model after the training has completed.  \n",
    "Here we check the baseline model perplexity, to have an idea how good the quantization is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do our magic - __Quantizing the model__.  \n",
    "The quantizer replaces the layers in out model with their quantized versions.  \n",
    "We can see that our model has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distiller.quantization import PostTrainLinearQuantizer, LinearQuantMode\n",
    "from copy import deepcopy\n",
    "# Define the quantizer\n",
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(rnn_model),\n",
    "    model_activation_stats='performance_rnn_pretrained_stats.yaml')\n",
    "\n",
    "# Quantizer magic:\n",
    "\n",
    "quantizer.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "\n",
    "def evaluate(model):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        # The line below was fixed as per: https://github.com/pytorch/examples/issues/214\n",
    "        for i in tqdm(range(0, data_source.size(0), sequence_len)):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(quantizer.model.to(device), val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the perplexity of the original and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Dataset\n",
    "from sequence import EventSeq\n",
    "import torch.functional as F\n",
    "\n",
    "data_path = \"dataset/processed/ecomp_piano\"\n",
    "dataset = Dataset(data_path, verbose=True)\n",
    "dataset_size = len(dataset.samples)\n",
    "assert dataset_size > 0\n",
    "\n",
    "# Eventually need to put these in YAML file.\n",
    "controls = None\n",
    "teacher_forcing_ratio = 1.0\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "nnl = nn.NLLLoss(reduction='sum')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "window_size = 200\n",
    "stride_size = 10\n",
    "use_transposition = False\n",
    "control_ratio = 1.0\n",
    "event_dim = EventSeq.dim()\n",
    "batch_size = 2\n",
    "\n",
    "batch_gen = dataset.batches(batch_size, window_size, stride_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n\nMismatch: 100%\nMax absolute difference: 3.35693359e-06\nMax relative difference: 4.72123396e-08\n x: array(71.10288, dtype=float32)\n y: array(71.102886)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-35a253679b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Check to make sure we're calculating the correct loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mnptest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/env/lib/python3.6/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[0;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[1;32m   1014\u001b[0m              \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m              precision=decimal)\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/env/lib/python3.6/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    825\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\nMismatch: 100%\nMax absolute difference: 3.35693359e-06\nMax relative difference: 4.72123396e-08\n x: array(71.10288, dtype=float32)\n y: array(71.102886)"
     ]
    }
   ],
   "source": [
    "import numpy.testing as nptest\n",
    "import pdb\n",
    "\n",
    "acc_loss = 0\n",
    "N = 0\n",
    "num_iters = 100\n",
    "\n",
    "for iteration, (events, controls) in enumerate(batch_gen):\n",
    "    if iteration == num_iters:\n",
    "        break\n",
    "        \n",
    "    if use_transposition:\n",
    "        offset = np.random.choice(np.arange(-6, 6))\n",
    "        events, controls = utils.transposition(events, controls, offset)\n",
    "\n",
    "    events = torch.LongTensor(events).to(device)\n",
    "    assert events.shape[0] == window_size\n",
    "\n",
    "    if np.random.random() < control_ratio:\n",
    "        controls = torch.FloatTensor(controls).to(device)\n",
    "        assert controls.shape[0] == window_size\n",
    "    else:\n",
    "        controls = None\n",
    "\n",
    "    init = torch.randn(batch_size, model.init_dim).to(device)\n",
    "    outputs = model.generate(init, window_size, events=events[:-1], controls=controls,\n",
    "                             teacher_forcing_ratio=teacher_forcing_ratio, output_type='logit')\n",
    "    \n",
    "\n",
    "    assert outputs.shape[:2] == events.shape[:2]\n",
    "\n",
    "    \n",
    "    loss1 = loss_function(outputs.view(-1, event_dim), events.view(-1))\n",
    "    pred = log_softmax(outputs.view(-1, event_dim))\n",
    "    n = pred.shape[0]\n",
    "    loss2 = nnl(pred, events.view(-1))\n",
    "    acc_loss += loss2\n",
    "    N += n\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "   \n",
    "    # Check to make sure we're calculating the correct loss.\n",
    "    nptest.assert_array_almost_equal(loss1.cpu().detach().numpy(), loss2.cpu().detach().numpy() / n)\n",
    "\n",
    "\n",
    "\n",
    "acc_cross_entropy_loss = acc_loss / N\n",
    "perplexity = acc_cross_entropy_loss.exp()\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(71.1029, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_loss / N"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
