{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PerformanceRNN\n",
    "import torch\n",
    "from torch import nn\n",
    "import distiller\n",
    "from distiller.modules.gru import DistillerGRU as GRU\n",
    "from distiller.modules.gru import convert_model_to_distiller_gru\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and converting to our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = 'cuda:0'\n",
    "sess_path = \"save/ecomp_w500.sess\"\n",
    "state = torch.load(sess_path)\n",
    "rnn_model = PerformanceRNN(**state['model_config']).to(device)\n",
    "rnn_model.load_state_dict(state['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): Embedding(240, 240)\n",
       "  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): DistillerGRU(512, 512, num_layers=3, dropout=0.30, bidirectional=False)\n",
       "  (output_fc): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_model_to_distiller_gru(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_model.gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that man_model is on GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(man_model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the conversion has succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to make sure that both the original and manual models can generate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "model = quantizer.model.to(device)\n",
    "model.eval()\n",
    "batch_size = 1\n",
    "init = torch.randn(batch_size, model.init_dim).to(device)\n",
    "max_len = 1000\n",
    "controls=None\n",
    "greedy_ratio = 0.7\n",
    "temperature = 1.0\n",
    "\n",
    "import pdb\n",
    "\n",
    "with torch.no_grad():\n",
    "    #pdb.set_trace()\n",
    "    outputs = model.generate(init, max_len,\n",
    "                             controls=controls,\n",
    "                             greedy=greedy_ratio,\n",
    "                             temperature=temperature,\n",
    "                             verbose=True)\n",
    "    \n",
    "\n",
    "outputs = outputs.cpu().numpy().T # [batch, steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert output to MIDI and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> quantized_output/output-000.mid (122 notes)\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import os\n",
    "\n",
    "output_dir = \"quantized_output/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    name = f'output-{i:03d}.mid'\n",
    "    path = os.path.join(output_dir, name)\n",
    "    n_notes = utils.event_indeces_to_midi_file(output, path)\n",
    "    print(f'===> {path} ({n_notes} notes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect activation statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses activation statistics to determine how big the quantization range is. The bigger the range - the larger the round off error after quantization which leads to accuracy drop.  \n",
    "Our goal is to minimize the range s.t. it contains the absolute most of our data.  \n",
    "After that, we divide the range into chunks of equal size, according to the number of bits, and transform the data according to this scale factor.  \n",
    "Read more on scale factor calculation [in our docs](https://nervanasystems.github.io/distiller/algo_quantization.html).\n",
    "\n",
    "The class `QuantCalibrationStatsCollector` collects the statistics for defining the range $r = max - min$.  \n",
    "\n",
    "Each forward pass, the collector records the values of inputs and outputs, for each layer:\n",
    "- absolute over all batches min, max (stored in `min`, `max`)\n",
    "- average over batches, per batch min, max (stored in `avg_min`, `avg_max`)\n",
    "- mean\n",
    "- std\n",
    "- shape of output tensor  \n",
    "\n",
    "All these values can be used to define the range of quantization, e.g. we can use the absolute `min`, `max` to define the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `man_model` has the same weights as `rnn_model` (Warning: Running this will move the models to the CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.testing as nptest\n",
    "\n",
    "man_model_weights = man_model.cpu().output_fc.weight.detach().numpy()\n",
    "rnn_model_weights = rnn_model.cpu().output_fc.weight.detach().numpy()\n",
    "nptest.assert_array_almost_equal(man_model_weights, rnn_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `man_model` is on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(man_model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# My version.\n",
    "import os\n",
    "from distiller.data_loggers import QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "# Commented line is probably not necessary.\n",
    "#man_model = torch.load('./manual.checkpoint.pth.tar')\n",
    "distiller.utils.assign_layer_fq_names(rnn_model)\n",
    "collector = QuantCalibrationStatsCollector(rnn_model)#,\n",
    "                                           #inplace_runtime_check=True,\n",
    "                                           #disable_inplace_attrs=True) # I added this last argument.\n",
    "\n",
    "# Random numbers.\n",
    "batch_size = 64\n",
    "max_len = 100\n",
    "\n",
    "if not os.path.isfile('performance_rnn_pretrained_stats.yaml'):\n",
    "    with collector_context(collector) as collector:\n",
    "        init = torch.randn(batch_size, rnn_model.init_dim).to(device)\n",
    "        output = rnn_model.generate(init, max_len)\n",
    "        collector.save('performance_rnn_pretrained_stats.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Model:\n",
    "  \n",
    "We quantize the model after the training has completed.  \n",
    "Here we check the baseline model perplexity, to have an idea how good the quantization is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do our magic - __Quantizing the model__.  \n",
    "The quantizer replaces the layers in out model with their quantized versions.  \n",
    "We can see that our model has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distiller.quantization import PostTrainLinearQuantizer, LinearQuantMode\n",
    "from copy import deepcopy\n",
    "# Define the quantizer\n",
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(rnn_model),\n",
    "    model_activation_stats='performance_rnn_pretrained_stats.yaml')\n",
    "\n",
    "# Quantizer magic:\n",
    "\n",
    "quantizer.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): RangeLinearQuantParamLayerWrapper(\n",
       "    mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "    preset_activation_stats=True\n",
       "    w_scale=520.7937, w_zero_point=0.0000\n",
       "    in_scale=35.3958, in_zero_point=0.0000\n",
       "    out_scale=28.3010, out_zero_point=0.0000\n",
       "    (wrapped_module): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  )\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): RangeLinearEmbeddingWrapper(\n",
       "    (wrapped_module): Embedding(240, 240)\n",
       "  )\n",
       "  (concat_input_fc): RangeLinearQuantParamLayerWrapper(\n",
       "    mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "    preset_activation_stats=True\n",
       "    w_scale=37.3165, w_zero_point=0.0000\n",
       "    in_scale=127.0000, in_zero_point=0.0000\n",
       "    out_scale=95.1984, out_zero_point=0.0000\n",
       "    (wrapped_module): Linear(in_features=265, out_features=512, bias=True)\n",
       "  )\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): DistillerGRU(512, 512, num_layers=3, dropout=0.30, bidirectional=False)\n",
       "  (output_fc): RangeLinearQuantParamLayerWrapper(\n",
       "    mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "    preset_activation_stats=True\n",
       "    w_scale=24.6316, w_zero_point=0.0000\n",
       "    in_scale=127.0001, in_zero_point=0.0000\n",
       "    out_scale=5.3074, out_zero_point=0.0000\n",
       "    (wrapped_module): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  )\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "\n",
    "def evaluate(model):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        # The line below was fixed as per: https://github.com/pytorch/examples/issues/214\n",
    "        for i in tqdm(range(0, data_source.size(0), sequence_len)):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(quantizer.model.to(device), val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the perplexity of the original and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Dataset\n",
    "from sequence import EventSeq\n",
    "\n",
    "data_path = \"dataset/processed/ecomp_piano\"\n",
    "dataset = Dataset(data_path, verbose=True)\n",
    "dataset_size = len(dataset.samples)\n",
    "assert dataset_size > 0\n",
    "\n",
    "# Eventually need to put these in YAML file.\n",
    "controls = None\n",
    "teacher_forcing_ratio = 1.0\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "window_size = 200\n",
    "stride_size = 10\n",
    "use_transposition = False\n",
    "control_ratio = 1.0\n",
    "event_dim = EventSeq.dim()\n",
    "\n",
    "batch_gen = dataset.batches(batch_size, window_size, stride_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor(2.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 0, loss: 2.171393394470215\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 1, loss: 2.1822714805603027\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 2, loss: 2.1458685398101807\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 3, loss: 2.2376677989959717\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 4, loss: 2.1788384914398193\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 5, loss: 2.203986406326294\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 6, loss: 2.1233153343200684\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 7, loss: 2.177565574645996\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 8, loss: 2.1893439292907715\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 9, loss: 2.1805076599121094\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 10, loss: 2.169264316558838\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 11, loss: 2.1641623973846436\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 12, loss: 2.177366018295288\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 13, loss: 2.2078299522399902\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 14, loss: 2.209015130996704\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 15, loss: 2.2081143856048584\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 16, loss: 2.209188938140869\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 17, loss: 2.2268121242523193\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 18, loss: 2.166034460067749\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 19, loss: 2.1588170528411865\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 20, loss: 2.197373867034912\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 21, loss: 2.1189324855804443\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 22, loss: 2.1816294193267822\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 23, loss: 2.1705808639526367\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 24, loss: 2.2396280765533447\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 25, loss: 2.1895864009857178\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 26, loss: 2.1899001598358154\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 27, loss: 2.190157890319824\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 28, loss: 2.1923019886016846\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 29, loss: 2.1475677490234375\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 30, loss: 2.1089160442352295\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 31, loss: 2.1871726512908936\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 32, loss: 2.153200149536133\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 33, loss: 2.2586939334869385\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 34, loss: 2.2001945972442627\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 35, loss: 2.1749417781829834\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 36, loss: 2.1613566875457764\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 37, loss: 2.1761155128479004\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 38, loss: 2.17576003074646\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 39, loss: 2.2403135299682617\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 40, loss: 2.196735143661499\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 41, loss: 2.2455174922943115\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 42, loss: 2.2446675300598145\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 43, loss: 2.1835362911224365\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 44, loss: 2.1371259689331055\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 45, loss: 2.1844987869262695\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 46, loss: 2.234713554382324\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 47, loss: 2.202644109725952\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 48, loss: 2.1799373626708984\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 49, loss: 2.1790921688079834\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 50, loss: 2.177212953567505\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 51, loss: 2.1801812648773193\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 52, loss: 2.1824393272399902\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 53, loss: 2.244635581970215\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 54, loss: 2.165709972381592\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 55, loss: 2.0950090885162354\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 56, loss: 2.214859962463379\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 57, loss: 2.245457887649536\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 58, loss: 2.1718053817749023\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 59, loss: 2.152968168258667\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 60, loss: 2.1855080127716064\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 61, loss: 2.2136926651000977\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 62, loss: 2.2157809734344482\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 63, loss: 2.117096424102783\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 64, loss: 2.214325189590454\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 65, loss: 2.158846139907837\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 66, loss: 2.1254405975341797\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 67, loss: 2.1754791736602783\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 68, loss: 2.1526267528533936\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 69, loss: 2.15934419631958\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 70, loss: 2.1669960021972656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor(2.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 71, loss: 2.1756279468536377\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 72, loss: 2.1943514347076416\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 73, loss: 2.206209659576416\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 74, loss: 2.210500717163086\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 75, loss: 2.17990779876709\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 76, loss: 2.2082276344299316\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 77, loss: 2.2238588333129883\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 78, loss: 2.1077404022216797\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 79, loss: 2.1044888496398926\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 80, loss: 2.132850170135498\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 81, loss: 2.174504280090332\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 82, loss: 2.2094979286193848\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 83, loss: 2.2312676906585693\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 84, loss: 2.2064011096954346\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 85, loss: 2.1593048572540283\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 86, loss: 2.1938817501068115\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 87, loss: 2.195899724960327\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 88, loss: 2.214113712310791\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 89, loss: 2.197288990020752\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 90, loss: 2.1311521530151367\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 91, loss: 2.185614585876465\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 92, loss: 2.1630396842956543\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 93, loss: 2.231163740158081\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 94, loss: 2.219698905944824\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 95, loss: 2.213606595993042\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 96, loss: 2.1646933555603027\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 97, loss: 2.1417746543884277\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 98, loss: 2.159001588821411\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 99, loss: 2.17720365524292\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 100, loss: 2.158156156539917\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 101, loss: 2.181792974472046\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 102, loss: 2.1705281734466553\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 103, loss: 2.1351094245910645\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 104, loss: 2.2017409801483154\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 105, loss: 2.206437587738037\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 106, loss: 2.168802261352539\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 107, loss: 2.221587896347046\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 108, loss: 2.1705873012542725\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 109, loss: 2.256643772125244\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 110, loss: 2.1914682388305664\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 111, loss: 2.170180082321167\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 112, loss: 2.1701319217681885\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 113, loss: 2.1810402870178223\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 114, loss: 2.206709623336792\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 115, loss: 2.196937322616577\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 116, loss: 2.2216317653656006\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 117, loss: 2.132951498031616\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 118, loss: 2.1845085620880127\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 119, loss: 2.175638437271118\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 120, loss: 2.1743204593658447\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 121, loss: 2.1637461185455322\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 122, loss: 2.1925601959228516\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 123, loss: 2.1910133361816406\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 124, loss: 2.158027410507202\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 125, loss: 2.193113327026367\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 126, loss: 2.169929265975952\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 127, loss: 2.1670217514038086\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 128, loss: 2.1826744079589844\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 129, loss: 2.1764822006225586\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 130, loss: 2.2189574241638184\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 131, loss: 2.2015910148620605\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 132, loss: 2.1969962120056152\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 133, loss: 2.177433729171753\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 134, loss: 2.1952033042907715\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 135, loss: 2.228682041168213\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 136, loss: 2.213670253753662\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 137, loss: 2.2451906204223633\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 138, loss: 2.1355271339416504\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 139, loss: 2.2134578227996826\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 140, loss: 2.142505645751953\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 141, loss: 2.200078010559082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor(2.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 142, loss: 2.179039716720581\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 143, loss: 2.1725101470947266\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 144, loss: 2.179532527923584\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 145, loss: 2.212005615234375\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 146, loss: 2.2406809329986572\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 147, loss: 2.1947851181030273\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 148, loss: 2.231478214263916\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 149, loss: 2.180799722671509\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 150, loss: 2.154621124267578\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 151, loss: 2.1536378860473633\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 152, loss: 2.1968119144439697\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 153, loss: 2.1855897903442383\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 154, loss: 2.1603782176971436\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 155, loss: 2.229729652404785\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 156, loss: 2.1487443447113037\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 157, loss: 2.1522371768951416\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 158, loss: 2.181913375854492\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 159, loss: 2.1445910930633545\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 160, loss: 2.2069919109344482\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 161, loss: 2.1842610836029053\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 162, loss: 2.2082996368408203\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 163, loss: 2.180621862411499\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 164, loss: 2.208487033843994\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 165, loss: 2.2125911712646484\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 166, loss: 2.1366994380950928\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 167, loss: 2.1878180503845215\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 168, loss: 2.1794381141662598\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 169, loss: 2.216416835784912\n",
      "<class 'torch.Tensor'>\n",
      "tensor(2.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "iter 170, loss: 2.1818699836730957\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-750b9c42e2a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     outputs = model.generate(init, window_size, events=events[:-1], controls=controls,\n\u001b[0;32m---> 17\u001b[0;31m                              teacher_forcing_ratio=teacher_forcing_ratio, output_type='logit')\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, init, steps, events, controls, greedy, user_events, temperature, teacher_forcing_ratio, output_type, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0muse_greedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, event, control, hidden)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_input_fc_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/distiller/modules/gru.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;31m# A new forward function to match the torch GRU API.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/distiller/modules/gru.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/distiller/modules/gru.py\u001b[0m in \u001b[0;36mprocess_layer_wise\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_chain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/distiller/modules/gru.py\u001b[0m in \u001b[0;36m_layer_chain_unidirectional\u001b[0;34m(self, step, h)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/distiller/modules/gru.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     94\u001b[0m         h = self.eltwiseadd_gate(\n\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meltwisemult_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_minus_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meltwisemult_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Performance-RNN-PyTorch/distiller/distiller/modules/eltwise.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration, (events, controls) in enumerate(batch_gen):\n",
    "    if use_transposition:\n",
    "        offset = np.random.choice(np.arange(-6, 6))\n",
    "        events, controls = utils.transposition(events, controls, offset)\n",
    "\n",
    "    events = torch.LongTensor(events).to(device)\n",
    "    assert events.shape[0] == window_size\n",
    "\n",
    "    if np.random.random() < control_ratio:\n",
    "        controls = torch.FloatTensor(controls).to(device)\n",
    "        assert controls.shape[0] == window_size\n",
    "    else:\n",
    "        controls = None\n",
    "\n",
    "    init = torch.randn(batch_size, model.init_dim).to(device)\n",
    "    outputs = model.generate(init, window_size, events=events[:-1], controls=controls,\n",
    "                             teacher_forcing_ratio=teacher_forcing_ratio, output_type='logit')\n",
    "    assert outputs.shape[:2] == events.shape[:2]\n",
    "\n",
    "    loss = loss_function(outputs.view(-1, event_dim), events.view(-1))\n",
    "    print(loss)\n",
    "    #model.zero_grad()\n",
    "    #loss.backward()\n",
    "\n",
    "#    norm = utils.compute_gradient_norm(model.parameters())\n",
    "#    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    #optimizer.step()\n",
    "\n",
    "#    if enable_logging:\n",
    "#        writer.add_scalar('model/loss', loss.item(), iteration)\n",
    "#        writer.add_scalar('model/norm', norm.item(), iteration)\n",
    "\n",
    "    print(f'iter {iteration}, loss: {loss.item()}')\n",
    "\n",
    "#    if time.time() - last_saving_time > saving_interval:\n",
    "#        save_model()\n",
    "#        last_saving_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): Embedding(240, 240)\n",
       "  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): DistillerGRU(512, 512, num_layers=3, dropout=0.30, bidirectional=False)\n",
       "  (output_fc): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
