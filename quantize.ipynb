{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing RNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how to quantize recurrent models.  \n",
    "Using a pretrained model `model.RNNModel`, we convert the built-in pytorch implementation of LSTM to our own, modular implementation.  \n",
    "The pretrained model was generated with:  \n",
    "```time python3 main.py --cuda --emsize 1500 --nhid 1500 --dropout 0.65 --tied --wd=1e-6```  \n",
    "The reason we replace the LSTM that is because the inner operations in the pytorch implementation are not accessible to us, but we still want to quantize these operations. <br />\n",
    "Afterwards we can try different techniques to quantize the whole model.  \n",
    "\n",
    "_NOTE_: We use `tqdm` to plot progress bars, since it's not in `requirements.txt` you should install it using \n",
    "`pip install tqdm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PerformanceRNN\n",
    "from distiller_model import DistillerPerformanceRNN\n",
    "import torch\n",
    "from torch import nn\n",
    "import distiller\n",
    "from distiller.modules import DistillerLSTM as LSTM\n",
    "from distiller.modules import convert_model_to_distiller_lstm\n",
    "from tqdm import tqdm # for pretty progress bar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data:\n",
    "\n",
    "Skip this, my data is already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('./data/wikitext-2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "device = 'cuda:0'\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and converting to our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original (keep for sake of comparison)\n",
    "rnn_model = torch.load('./checkpoint.pth.tar.best')\n",
    "rnn_model = rnn_model.to(device)\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = 'cuda:0'\n",
    "sess_path = \"save/LSTM_model.sess\"\n",
    "state = torch.load(sess_path)\n",
    "rnn_model = PerformanceRNN(**state['model_config']).to(device)\n",
    "rnn_model.load_state_dict(state['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out something I saw in the source code.\n",
    "man_model = convert_model_to_distiller_lstm(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillerLSTM(512, 512, num_layers=3, dropout=0.30, bidirectional=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_model.gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the pytorch LSTM implementation to our own, by calling `LSTM.from_pytorch_impl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original, I think I did this with convert_model_to_distiller_lstm above.\n",
    "def manual_model(pytorch_model_: 'RNNModel'):\n",
    "    nlayers, ninp, nhid, ntoken, tie_weights = \\\n",
    "        pytorch_model_.nlayers, \\\n",
    "        pytorch_model_.ninp, \\\n",
    "        pytorch_model_.nhid, \\\n",
    "        pytorch_model_.ntoken, \\\n",
    "        pytorch_model_.tie_weights\n",
    "\n",
    "    model = DistillerRNNModel(nlayers=nlayers, ninp=ninp, nhid=nhid, ntoken=ntoken, tie_weights=tie_weights).to(device)\n",
    "    model.eval()\n",
    "    model.encoder.weight = nn.Parameter(pytorch_model_.encoder.weight.clone().detach())\n",
    "    model.decoder.weight = nn.Parameter(pytorch_model_.decoder.weight.clone().detach())\n",
    "    model.decoder.bias = nn.Parameter(pytorch_model_.decoder.bias.clone().detach())\n",
    "    model.rnn = LSTM.from_pytorch_impl(pytorch_model_.rnn)\n",
    "\n",
    "    return model\n",
    "\n",
    "man_model = manual_model(rnn_model)\n",
    "torch.save(man_model, 'manual.checkpoint.pth.tar')\n",
    "man_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the data for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "sequence_len = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(sequence_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "hidden = rnn_model.init_hidden(eval_batch_size)\n",
    "data, targets = get_batch(test_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(sequence_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "\n",
    "hidden = rnn_model.init_hidden(eval_batch_size)\n",
    "data, targets = get_batch(test_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the convertion has succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): Embedding(240, 240)\n",
       "  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): DistillerLSTM(512, 512, num_layers=3, dropout=0.30, bidirectional=False)\n",
       "  (output_fc): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): Embedding(240, 240)\n",
       "  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): DistillerLSTM(512, 512, num_layers=3, dropout=0.30, bidirectional=False)\n",
       "  (output_fc): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t, h_t = rnn_model(data, hidden)\n",
    "y_p, h_p = man_model(data, hidden)\n",
    "\n",
    "print(\"Max error in y: %f\" % (y_t-y_p).abs().max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "\n",
    "def evaluate(model, data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        # The line below was fixed as per: https://github.com/pytorch/examples/issues/214\n",
    "        for i in tqdm(range(0, data_source.size(0), sequence_len)):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect activation statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses activation statistics to determine how big the quantization range is. The bigger the range - the larger the round off error after quantization which leads to accuracy drop.  \n",
    "Our goal is to minimize the range s.t. it contains the absolute most of our data.  \n",
    "After that, we divide the range into chunks of equal size, according to the number of bits, and transform the data according to this scale factor.  \n",
    "Read more on scale factor calculation [in our docs](https://nervanasystems.github.io/distiller/algo_quantization.html).\n",
    "\n",
    "The class `QuantCalibrationStatsCollector` collects the statistics for defining the range $r = max - min$.  \n",
    "\n",
    "Each forward pass, the collector records the values of inputs and outputs, for each layer:\n",
    "- absolute over all batches min, max (stored in `min`, `max`)\n",
    "- average over batches, per batch min, max (stored in `avg_min`, `avg_max`)\n",
    "- mean\n",
    "- std\n",
    "- shape of output tensor  \n",
    "\n",
    "All these values can be used to define the range of quantization, e.g. we can use the absolute `min`, `max` to define the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original.\n",
    "import os\n",
    "from distiller.data_loggers import QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "man_model = torch.load('./manual.checkpoint.pth.tar')\n",
    "distiller.utils.assign_layer_fq_names(man_model)\n",
    "collector = QuantCalibrationStatsCollector(man_model)\n",
    "\n",
    "if not os.path.isfile('manual_lstm_pretrained_stats.yaml'):\n",
    "    with collector_context(collector) as collector:\n",
    "        val_loss = evaluate(man_model, val_data)\n",
    "        collector.save('manual_lstm_pretrained_stats.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = 'cuda:0'\n",
    "sess_path = \"save/LSTM_model.sess\"\n",
    "state = torch.load(sess_path)\n",
    "man_model = PerformanceRNN(**state['model_config']).to(device)\n",
    "rnn_model.load_state_dict(state['model_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `man_model` has the same weights as `rnn_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1581,  0.1355,  0.1228,  ...,  0.0996, -0.0048,  0.2377],\n",
       "        [-0.1011,  0.0526,  0.0331,  ...,  0.0476, -0.0766, -0.0931],\n",
       "        [-0.0936,  0.1523,  0.0625,  ...,  0.0372, -0.0348, -0.0918],\n",
       "        ...,\n",
       "        [-0.0509,  0.0499,  0.0362,  ...,  0.1872, -0.0104, -0.0966],\n",
       "        [ 0.0243, -0.0208,  0.0151,  ...,  0.2488, -0.0781, -0.1401],\n",
       "        [-0.0366,  0.1303,  0.0476,  ...,  0.1466, -0.0225, -0.0725]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_model.output_fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1581,  0.1355,  0.1228,  ...,  0.0996, -0.0048,  0.2377],\n",
       "        [-0.1011,  0.0526,  0.0331,  ...,  0.0476, -0.0766, -0.0931],\n",
       "        [-0.0936,  0.1523,  0.0625,  ...,  0.0372, -0.0348, -0.0918],\n",
       "        ...,\n",
       "        [-0.0509,  0.0499,  0.0362,  ...,  0.1872, -0.0104, -0.0966],\n",
       "        [ 0.0243, -0.0208,  0.0151,  ...,  0.2488, -0.0781, -0.1401],\n",
       "        [-0.0366,  0.1303,  0.0476,  ...,  0.1466, -0.0225, -0.0725]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.output_fc.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `man_model` is on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(man_model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My version.\n",
    "import os\n",
    "from distiller.data_loggers import QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "# Commented line is probably not necessary.\n",
    "#man_model = torch.load('./manual.checkpoint.pth.tar')\n",
    "distiller.utils.assign_layer_fq_names(man_model)\n",
    "collector = QuantCalibrationStatsCollector(man_model)\n",
    "\n",
    "# Random numbers.\n",
    "batch_size = 64\n",
    "max_len = 100\n",
    "\n",
    "if not os.path.isfile('performance_rnn_pretrained_stats.yaml'):\n",
    "    with collector_context(collector) as collector:\n",
    "        init = torch.randn(batch_size, man_model.init_dim).to(device)\n",
    "        output = man_model.generate(init, max_len)\n",
    "        collector.save('performance_rnn_pretrained_stats.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_model.init_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Model:\n",
    "  \n",
    "We quantize the model after the training has completed.  \n",
    "Here we check the baseline model perplexity, to have an idea how good the quantization is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distiller.quantization import PostTrainLinearQuantizer, LinearQuantMode\n",
    "from copy import deepcopy\n",
    "\n",
    "# Load and evaluate the baseline model.\n",
    "man_model = torch.load('./manual.checkpoint.pth.tar')\n",
    "val_loss = evaluate(man_model, val_data)\n",
    "print('val_loss:%8.2f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do our magic - __Quantizing the model__.  \n",
    "The quantizer replaces the layers in out model with their quantized versions.  \n",
    "We can see that our model has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distiller.quantization import PostTrainLinearQuantizer, LinearQuantMode\n",
    "from copy import deepcopy\n",
    "# Define the quantizer\n",
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='performance_rnn_pretrained_stats.yaml')\n",
    "\n",
    "# Quantizer magic:\n",
    "quantizer.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): RangeLinearQuantParamLayerWrapper(\n",
       "    mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "    preset_activation_stats=True\n",
       "    w_scale=854.0115, w_zero_point=0.0000\n",
       "    in_scale=38.1417, in_zero_point=0.0000\n",
       "    out_scale=92.7936, out_zero_point=0.0000\n",
       "    (wrapped_module): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  )\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): RangeLinearEmbeddingWrapper(\n",
       "    (wrapped_module): Embedding(240, 240)\n",
       "  )\n",
       "  (concat_input_fc): RangeLinearQuantParamLayerWrapper(\n",
       "    mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "    preset_activation_stats=True\n",
       "    w_scale=91.4019, w_zero_point=0.0000\n",
       "    in_scale=127.0000, in_zero_point=0.0000\n",
       "    out_scale=47.6283, out_zero_point=0.0000\n",
       "    (wrapped_module): Linear(in_features=265, out_features=512, bias=True)\n",
       "  )\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): DistillerLSTM(512, 512, num_layers=3, dropout=0.30, bidirectional=False)\n",
       "  (output_fc): RangeLinearQuantParamLayerWrapper(\n",
       "    mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "    preset_activation_stats=True\n",
       "    w_scale=46.6254, w_zero_point=0.0000\n",
       "    in_scale=127.0033, in_zero_point=0.0000\n",
       "    out_scale=7.1114, out_zero_point=0.0000\n",
       "    (wrapped_module): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  )\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = evaluate(quantizer.model.to(device), val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('val_loss:%8.2f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the perplexity has increased much - meaning our quantization has damaged the accuracy of our model.  \n",
    "Let's try quantizing each channel separately, and making the range of the quantization asymmetric.  \n",
    "Also - we replaced the `min`, `max` boundaries manually in the file.  \n",
    "The idea is - the quantizer takes the absolute `min`, `max` boundaries by default, and in the original file many of the activations had a very large range that makes our quants very big - while we want to minimize their size since each quant corresponds to a roundoff error.  \n",
    "The activations in every LSTM are either `sigmoid` or `tanh`, and since these are bounded respectively by\n",
    "$[0,1]$, $[-1,1]$ and they saturate very quickly - we can clip the inputs to be between in the range of $[-6,6]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats_new.yaml',\n",
    "    mode=LinearQuantMode.ASYMMETRIC_SIGNED,\n",
    "    per_channel_wts=True\n",
    ")\n",
    "quantizer.prepare_model()\n",
    "quantizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = evaluate(quantizer.model.to(device), val_data)\n",
    "print('val_loss:%8.2f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tiny bit better, but still no good. Let us try the half precision version of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp16 = deepcopy(man_model).half()\n",
    "val_loss = evaluate(model_fp16, val_data)\n",
    "print('val_loss: %8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is very close to our original model! That means that the roundoff when quantizing lineary is what hurts our accuracy. Let's try then quantizing everything except elemtentwise operations, as stated in \n",
    "[`Effective Quantization Methods for Recurrent Neural Networks`](https://arxiv.org/abs/1611.10176) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides_yaml = \"\"\"\n",
    ".*eltwise.*:\n",
    "    fp16: true\n",
    "encoder:\n",
    "    fp16: true\n",
    "decoder:\n",
    "    fp16: true\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats_new.yaml',\n",
    "    mode=LinearQuantMode.ASYMMETRIC_SIGNED,\n",
    "    overrides=overrides,\n",
    "    per_channel_wts=True\n",
    ")\n",
    "quantizer.prepare_model()\n",
    "val_loss = evaluate(quantizer.model.to(device), val_data)\n",
    "print('val_loss:%8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is still holding up very well, even though we quantized the inner linear layers!  \n",
    "Now, lets try to choose different boundaries for `min`, `max` -  \n",
    "Instead of using absolute ones, we take the average of all batches (`avg_min`, `avg_max`), which is an indication of where usually most of the boundaries lie. This is done by specifying the `clip_acts` parameter to `ClipMode.AVG` or `\"AVG\"` in the quantizer ctor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides_yaml = \"\"\"\n",
    "encoder:\n",
    "    fp16: true\n",
    "decoder:\n",
    "    fp16: true\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats.yaml',\n",
    "    mode=LinearQuantMode.ASYMMETRIC_SIGNED,\n",
    "    overrides=overrides,\n",
    "    per_channel_wts=True,\n",
    "    clip_acts=\"AVG\"\n",
    ")\n",
    "quantizer.prepare_model()\n",
    "val_loss = evaluate(quantizer.model.to(device), val_data)\n",
    "print('val_loss:%8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Even though we quantized all of the layers except the embedding and the decoder - we got almost no accuracy penalty. Lets try quantizing them as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats_new.yaml',\n",
    "    mode=LinearQuantMode.ASYMMETRIC_SIGNED,\n",
    "    per_channel_wts=True,\n",
    "    clip_acts=\"AVG\"\n",
    ")\n",
    "quantizer.prepare_model()\n",
    "val_loss = evaluate(quantizer.model.to(device), val_data)\n",
    "print('val_loss:%8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that sometimes quantizing with the right boundaries gives better results than actually using floating point operations (even though they are half precision). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Choosing the right boundaries for quantization  was crucial for achieving almost no degradation in accrucay of LSTM.  \n",
    "  \n",
    "Here we showed how to use the distiller quantization API to quantize an RNN model, by converting the pytorch implementation into a modular one and then quantizing each layer separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
