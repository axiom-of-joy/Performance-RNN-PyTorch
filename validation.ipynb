{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of Amadeus\n",
    "\n",
    "This notebook compares the perplexity of quantized and unquantized instances of a trained Performance RNN model in order to validate the compressed model.\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "As described in the [Wikipedia article](https://en.wikipedia.org/wiki/Perplexity), perplexity is a \"measurement of how well a probability distribution or probability model predicts a sample.\" If \\\\(T\\\\) is a test set and \\\\(q\\\\) is the distribution predicted by the model, then the empircal perplexity is given by\n",
    "\n",
    "\\\\[\n",
    "2^{H(T, q)}\n",
    "\\\\]\n",
    "\n",
    "where\n",
    "\n",
    "\\\\[\n",
    "H(T, q) = -\\frac{1}{N} \\sum_{i=1}^N \\log_2 q(x_i)\n",
    "\\\\]\n",
    "\n",
    "is the empical cross-entropy on the test set \\\\(T\\\\). Note that a lower perplexity is a good thing; it means that the model is better able to predict a sample.\n",
    "\n",
    "## Load Model\n",
    "\n",
    "The following code loads a trained instance of Performance RNN and computes a quantized instance of the same model. It requires that the trained weights in `ecomp_w500.sess` are located in the `save/` folder and that the quantization calibration statistics are stored in a file `performance_rnn_pretrained_stats.yaml` in the `stats/` folder. It also assumes that the [MAESTRO dataset](https://magenta.tensorflow.org/datasets/maestro) has been downloaded and preprocessed (see the [Usage section of the README](https://github.com/axiom-of-joy/amadeus#usage) for preprocessing instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as nptest\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "from tqdm import tqdm\n",
    "import distiller\n",
    "from distiller.modules.gru import convert_model_to_distiller_gru\n",
    "import config\n",
    "from data import Dataset\n",
    "from model import PerformanceRNN\n",
    "from quantize import Quantizer\n",
    "from sequence import EventSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "sess_path = \"save/ecomp_w500.sess\"\n",
    "#stats_file = \"stats/quant_stats.yaml\"\n",
    "stats_file = \"stats/performance_rnn_pretrained_stats.yaml\"\n",
    "\n",
    "# Create an instance of Performance RNN and load pre-trained weights.\n",
    "state = torch.load(sess_path)\n",
    "model = PerformanceRNN(**state['model_config']).to(device)\n",
    "model.load_state_dict(state['model_state'])\n",
    "\n",
    "# Obtain the quantized model.\n",
    "Q = Quantizer(model)\n",
    "quant_model = Q.quantize(stats_file).model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next call `eval` on both `model` and `quant_model` to disable Dropout. Note that the GRU in `model` is the usual `torch.nn.modules.GRU`, while the GRU in `quant_model` is an instance of `DistillerGRU`, the quantized GRU class I created in my forked Distiller repository. Also note that many of the linear and activation layers in the original model have been replaced by instances of `RangeLinearQuantParamLayerWrapper`, a Distiller class that takes care of quantizing inputs and de-quantizing outputs to the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): Embedding(240, 240)\n",
       "  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): GRU(512, 512, num_layers=3, dropout=0.3)\n",
       "  (output_fc): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceRNN(\n",
       "  (inithid_fc): FP16Wrapper(\n",
       "    (wrapped_module): Linear(in_features=32, out_features=1536, bias=True)\n",
       "  )\n",
       "  (inithid_fc_activation): Tanh()\n",
       "  (event_embedding): FP16Wrapper(\n",
       "    (wrapped_module): Embedding(240, 240)\n",
       "  )\n",
       "  (concat_input_fc): FP16Wrapper(\n",
       "    (wrapped_module): Linear(in_features=265, out_features=512, bias=True)\n",
       "  )\n",
       "  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  (gru): DistillerGRU(512, 512, num_layers=3, dropout=0.30, bidirectional=False)\n",
       "  (output_fc): FP16Wrapper(\n",
       "    (wrapped_module): Linear(in_features=1536, out_features=240, bias=True)\n",
       "  )\n",
       "  (output_fc_activation): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Model Size\n",
    "\n",
    "The following computes the number of parameters in the original model Performance RNN. The quantized model contains the same number of parameters, but each parameter is an 8-bit int rather than a 32-bit float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('inithid_fc',\n",
       "              Linear(in_features=32, out_features=1536, bias=True)),\n",
       "             ('inithid_fc_activation', Tanh()),\n",
       "             ('event_embedding', Embedding(240, 240)),\n",
       "             ('concat_input_fc',\n",
       "              Linear(in_features=265, out_features=512, bias=True)),\n",
       "             ('concat_input_fc_activation',\n",
       "              LeakyReLU(negative_slope=0.1, inplace)),\n",
       "             ('gru', GRU(512, 512, num_layers=3, dropout=0.3)),\n",
       "             ('output_fc',\n",
       "              Linear(in_features=1536, out_features=240, bias=True)),\n",
       "             ('output_fc_activation', Softmax())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368880\n"
     ]
    }
   ],
   "source": [
    "sigma = 0\n",
    "for p in model._modules['output_fc'].parameters():\n",
    "    sigma += p.numel()\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5341168"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the perplexity of the original and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set (MAESTRO dataset was not used for training or validation).\n",
    "data_path = \"dataset/processed/maestro\"\n",
    "dataset = Dataset(data_path, verbose=True)\n",
    "dataset_size = len(dataset.samples)\n",
    "assert dataset_size > 0\n",
    "\n",
    "# Define parameters for generation.\n",
    "# (Same as for collecting quantization calibration statitics).\n",
    "#controls = None\n",
    "event_dim = EventSeq.dim()\n",
    "num_iters = 100\n",
    "batch_size = config.collect_quant_stats['batch_size']\n",
    "window_size = config.collect_quant_stats['window_size']\n",
    "stride_size = config.collect_quant_stats['stride_size']\n",
    "use_transposition = config.collect_quant_stats['use_transposition']\n",
    "control_ratio = config.collect_quant_stats['control_ratio']\n",
    "teacher_forcing_ratio = config.collect_quant_stats['teacher_forcing_ratio']\n",
    "\n",
    "# Create batch generator.\n",
    "batch_gen = dataset.batches(batch_size, window_size, stride_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for computing the empirical perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(model, batch_gen, num_iters, event_dim,\n",
    "                       batch_size, window_size, use_transposition,\n",
    "                       control_ratio, teacher_forcing_ratio):\n",
    "    \"\"\"\n",
    "    Computes the empirical perplexity of model on the data in batch_gen.\n",
    "    \n",
    "    Args:\n",
    "        model (Performance RNN): A quantized or unquantized instance of\n",
    "            Performance RNN.\n",
    "        batch_gen: A batch generator created from an instance of Dataset.\n",
    "        num_iters (int): Number of iterations through batch_gen.\n",
    "        event_dim (int): Dimension of EventSeq object.\n",
    "        batch_size (int): Batch size.\n",
    "        window_size (int): Number of note events in window when predicting sample.\n",
    "        use_transposition (bool): Indicates whether to transpose inputs.\n",
    "        control_ratio (float): Control ratio.\n",
    "        teacher_forcing_ratio (float): Teacher forcing ratio.\n",
    "        \n",
    "    Returns:\n",
    "        perplexity (float): Empirical perplexity of model on data in batch_gen.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define loss functions.\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "    nnl = nn.NLLLoss(reduction='sum')\n",
    "    \n",
    "    # Accumulated loss and number of samples.\n",
    "    acc_loss = 0\n",
    "    N = 0\n",
    "\n",
    "    for iteration in tqdm(range(num_iters)):\n",
    "        events, controls = next(batch_gen)\n",
    "\n",
    "        if use_transposition:\n",
    "            offset = np.random.choice(np.arange(-6, 6))\n",
    "            events, controls = utils.transposition(events, controls, offset)\n",
    "\n",
    "        events = torch.LongTensor(events).to(device)\n",
    "        assert events.shape[0] == window_size\n",
    "\n",
    "        if np.random.random() < control_ratio:\n",
    "            controls = torch.FloatTensor(controls).to(device)\n",
    "            assert controls.shape[0] == window_size\n",
    "        else:\n",
    "            controls = None\n",
    "\n",
    "        init = torch.randn(batch_size, model.init_dim).to(device)\n",
    "        outputs = model.generate(init, window_size, events=events[:-1], controls=controls,\n",
    "                                 teacher_forcing_ratio=teacher_forcing_ratio, output_type='logit')\n",
    "\n",
    "        assert outputs.shape[:2] == events.shape[:2]\n",
    "\n",
    "        loss1 = loss_function(outputs.view(-1, event_dim), events.view(-1))\n",
    "        pred = log_softmax(outputs.view(-1, event_dim))\n",
    "        n = pred.shape[0]\n",
    "        loss2 = nnl(pred, events.view(-1))\n",
    "        acc_loss += loss2\n",
    "        N += n\n",
    "\n",
    "        # Check to make sure we're calculating the correct loss.\n",
    "        nptest.assert_array_almost_equal(loss1.cpu().detach().numpy(), loss2.cpu().detach().numpy() / n)\n",
    "\n",
    "    acc_cross_entropy_loss = acc_loss / N\n",
    "    perplexity = acc_cross_entropy_loss.exp()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the empirical perplexity for both the quantized and unquantized models (this may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  1.77it/s]\n",
      "100%|██████████| 100/100 [05:46<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "unquant_perplexity = compute_perplexity(model, batch_gen, num_iters, event_dim,\n",
    "                                        batch_size, window_size, use_transposition,\n",
    "                                        control_ratio, teacher_forcing_ratio)\n",
    "quant_perplexity = compute_perplexity(quant_model, batch_gen, num_iters, event_dim,\n",
    "                                      batch_size, window_size, use_transposition,\n",
    "                                      control_ratio, teacher_forcing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the quantized and unquantized perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.1709, device='cuda:0', grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unquant_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7300, device='cuda:0', grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_perplexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
